{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cancer_Breast_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgFKoh+3uRxcNi3KUPb6O/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royparth20/CO-LAB.GOOGLE/blob/master/Cancer_Breast_ANN%20with%20Hyper%20Parameter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2AZMk9HdQq7",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "1fdb6fb1-b2d7-47dd-b25b-64b0a5702478"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-910bb9ab-ed34-491c-a946-c53a855d4dfa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-910bb9ab-ed34-491c-a946-c53a855d4dfa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data.csv to data.csv\n",
            "User uploaded file \"data.csv\" with length 125204 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6SNIXIhd0ye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "428979d6-3bdb-4af0-c3b0-8160834e09c8"
      },
      "source": [
        "uploaded\n",
        "import pandas as pd\n",
        "import io\n",
        "data = pd.read_csv(io.StringIO(uploaded['data.csv'].decode('utf-8')))\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>843786</td>\n",
              "      <td>M</td>\n",
              "      <td>12.45</td>\n",
              "      <td>15.70</td>\n",
              "      <td>82.57</td>\n",
              "      <td>477.1</td>\n",
              "      <td>0.12780</td>\n",
              "      <td>0.17000</td>\n",
              "      <td>0.15780</td>\n",
              "      <td>0.08089</td>\n",
              "      <td>0.2087</td>\n",
              "      <td>0.07613</td>\n",
              "      <td>0.3345</td>\n",
              "      <td>0.8902</td>\n",
              "      <td>2.217</td>\n",
              "      <td>27.19</td>\n",
              "      <td>0.007510</td>\n",
              "      <td>0.03345</td>\n",
              "      <td>0.03672</td>\n",
              "      <td>0.01137</td>\n",
              "      <td>0.02165</td>\n",
              "      <td>0.005082</td>\n",
              "      <td>15.47</td>\n",
              "      <td>23.75</td>\n",
              "      <td>103.40</td>\n",
              "      <td>741.6</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.5249</td>\n",
              "      <td>0.5355</td>\n",
              "      <td>0.1741</td>\n",
              "      <td>0.3985</td>\n",
              "      <td>0.12440</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>844359</td>\n",
              "      <td>M</td>\n",
              "      <td>18.25</td>\n",
              "      <td>19.98</td>\n",
              "      <td>119.60</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>0.09463</td>\n",
              "      <td>0.10900</td>\n",
              "      <td>0.11270</td>\n",
              "      <td>0.07400</td>\n",
              "      <td>0.1794</td>\n",
              "      <td>0.05742</td>\n",
              "      <td>0.4467</td>\n",
              "      <td>0.7732</td>\n",
              "      <td>3.180</td>\n",
              "      <td>53.91</td>\n",
              "      <td>0.004314</td>\n",
              "      <td>0.01382</td>\n",
              "      <td>0.02254</td>\n",
              "      <td>0.01039</td>\n",
              "      <td>0.01369</td>\n",
              "      <td>0.002179</td>\n",
              "      <td>22.88</td>\n",
              "      <td>27.66</td>\n",
              "      <td>153.20</td>\n",
              "      <td>1606.0</td>\n",
              "      <td>0.1442</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>0.3784</td>\n",
              "      <td>0.1932</td>\n",
              "      <td>0.3063</td>\n",
              "      <td>0.08368</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>84458202</td>\n",
              "      <td>M</td>\n",
              "      <td>13.71</td>\n",
              "      <td>20.83</td>\n",
              "      <td>90.20</td>\n",
              "      <td>577.9</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0.16450</td>\n",
              "      <td>0.09366</td>\n",
              "      <td>0.05985</td>\n",
              "      <td>0.2196</td>\n",
              "      <td>0.07451</td>\n",
              "      <td>0.5835</td>\n",
              "      <td>1.3770</td>\n",
              "      <td>3.856</td>\n",
              "      <td>50.96</td>\n",
              "      <td>0.008805</td>\n",
              "      <td>0.03029</td>\n",
              "      <td>0.02488</td>\n",
              "      <td>0.01448</td>\n",
              "      <td>0.01486</td>\n",
              "      <td>0.005412</td>\n",
              "      <td>17.06</td>\n",
              "      <td>28.14</td>\n",
              "      <td>110.60</td>\n",
              "      <td>897.0</td>\n",
              "      <td>0.1654</td>\n",
              "      <td>0.3682</td>\n",
              "      <td>0.2678</td>\n",
              "      <td>0.1556</td>\n",
              "      <td>0.3196</td>\n",
              "      <td>0.11510</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>844981</td>\n",
              "      <td>M</td>\n",
              "      <td>13.00</td>\n",
              "      <td>21.82</td>\n",
              "      <td>87.50</td>\n",
              "      <td>519.8</td>\n",
              "      <td>0.12730</td>\n",
              "      <td>0.19320</td>\n",
              "      <td>0.18590</td>\n",
              "      <td>0.09353</td>\n",
              "      <td>0.2350</td>\n",
              "      <td>0.07389</td>\n",
              "      <td>0.3063</td>\n",
              "      <td>1.0020</td>\n",
              "      <td>2.406</td>\n",
              "      <td>24.32</td>\n",
              "      <td>0.005731</td>\n",
              "      <td>0.03502</td>\n",
              "      <td>0.03553</td>\n",
              "      <td>0.01226</td>\n",
              "      <td>0.02143</td>\n",
              "      <td>0.003749</td>\n",
              "      <td>15.49</td>\n",
              "      <td>30.73</td>\n",
              "      <td>106.20</td>\n",
              "      <td>739.3</td>\n",
              "      <td>0.1703</td>\n",
              "      <td>0.5401</td>\n",
              "      <td>0.5390</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.4378</td>\n",
              "      <td>0.10720</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>84501001</td>\n",
              "      <td>M</td>\n",
              "      <td>12.46</td>\n",
              "      <td>24.04</td>\n",
              "      <td>83.97</td>\n",
              "      <td>475.9</td>\n",
              "      <td>0.11860</td>\n",
              "      <td>0.23960</td>\n",
              "      <td>0.22730</td>\n",
              "      <td>0.08543</td>\n",
              "      <td>0.2030</td>\n",
              "      <td>0.08243</td>\n",
              "      <td>0.2976</td>\n",
              "      <td>1.5990</td>\n",
              "      <td>2.039</td>\n",
              "      <td>23.94</td>\n",
              "      <td>0.007149</td>\n",
              "      <td>0.07217</td>\n",
              "      <td>0.07743</td>\n",
              "      <td>0.01432</td>\n",
              "      <td>0.01789</td>\n",
              "      <td>0.010080</td>\n",
              "      <td>15.09</td>\n",
              "      <td>40.68</td>\n",
              "      <td>97.65</td>\n",
              "      <td>711.4</td>\n",
              "      <td>0.1853</td>\n",
              "      <td>1.0580</td>\n",
              "      <td>1.1050</td>\n",
              "      <td>0.2210</td>\n",
              "      <td>0.4366</td>\n",
              "      <td>0.20750</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302         M  ...                  0.11890          NaN\n",
              "1    842517         M  ...                  0.08902          NaN\n",
              "2  84300903         M  ...                  0.08758          NaN\n",
              "3  84348301         M  ...                  0.17300          NaN\n",
              "4  84358402         M  ...                  0.07678          NaN\n",
              "5    843786         M  ...                  0.12440          NaN\n",
              "6    844359         M  ...                  0.08368          NaN\n",
              "7  84458202         M  ...                  0.11510          NaN\n",
              "8    844981         M  ...                  0.10720          NaN\n",
              "9  84501001         M  ...                  0.20750          NaN\n",
              "\n",
              "[10 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc7FKAffej-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "846a5e34-b287-44c2-c66d-8439b89356e7"
      },
      "source": [
        "import seaborn as sns\n",
        "ax =  sns.countplot(data['diagnosis'])\n",
        "print(data['diagnosis'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "B    357\n",
            "M    212\n",
            "Name: diagnosis, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaTGmRGuf39v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "del data['Unnamed: 32']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4Fb2PgPgJaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X =  data.iloc[:,2:].values\n",
        "Y =  data.iloc[: ,1].values\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelEncoder = LabelEncoder()\n",
        "Y = labelEncoder.fit_transform(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k4Df_BU5N13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "97bc4d7b-4ee8-4cdd-d003-92214d4b22ff"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDqmBT9hiUzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25,random_state=0)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler=StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-o8qtGSkwpa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "23c9e675-71ed-4861-c984-38eb611043e8"
      },
      "source": [
        "Y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
              "       1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8Fjt5YKizw1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "458b1e13-8d80-4a8e-dccd-edeeba7d58b6"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.65079907, -0.43057322, -0.68024847, ..., -0.36433881,\n",
              "         0.32349851, -0.7578486 ],\n",
              "       [-0.82835341,  0.15226547, -0.82773762, ..., -1.45036679,\n",
              "         0.62563098, -1.03071387],\n",
              "       [ 1.68277234,  2.18977235,  1.60009756, ...,  0.72504581,\n",
              "        -0.51329768, -0.96601386],\n",
              "       ...,\n",
              "       [-1.33114223, -0.22172269, -1.3242844 , ..., -0.98806491,\n",
              "        -0.69995543, -0.12266325],\n",
              "       [-1.25110186, -0.24600763, -1.28700242, ..., -1.75887319,\n",
              "        -1.56206114, -1.00989735],\n",
              "       [-0.74662205,  1.14066273, -0.72203706, ..., -0.2860679 ,\n",
              "        -1.24094654,  0.2126516 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uh5WvBui2NF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "fc026ecc-d67b-4f86-8138-b6f66b55f8ea"
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVMOOoAlghO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4bc1aec6-5981-4b89-ec4e-f68f703afe7f"
      },
      "source": [
        "import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khy2Pzvpnh8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "c0ab4872-a8a0-4513-8357-f467a4353905"
      },
      "source": [
        "classifier = Sequential()\n",
        "#First Layer (Input + Hidden Layer)\n",
        "classifier.add(Dense(output_dim = 16,init='uniform',activation='relu',input_dim=30 ))\n",
        "#Second Layer (Hidden Layer)\n",
        "classifier.add(Dense(output_dim = 16,init='uniform',activation='relu'))\n",
        "#Third Layer (Output Layer)\n",
        "classifier.add(Dense(output_dim = 1,init='uniform',activation=\"sigmoid\" ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=30, units=16, kernel_initializer=\"uniform\")`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=16, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrz2EEEKwPBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.compile(optimizer=\"Adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xNrfqxCuluk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7959657-7973-4b55-a849-40593b3d4c5e"
      },
      "source": [
        "classifier.weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense_1/kernel:0' shape=(30, 16) dtype=float32, numpy=\n",
              " array([[-4.35644872e-02, -2.90952325e-02, -3.33251953e-02,\n",
              "         -2.56953482e-02, -3.01806927e-02,  9.17050987e-03,\n",
              "         -2.90612709e-02,  3.16110738e-02,  1.87874921e-02,\n",
              "          5.15241548e-03,  3.63616608e-02, -2.53002886e-02,\n",
              "         -3.68773341e-02, -2.02178359e-02,  4.82880212e-02,\n",
              "         -4.48982008e-02],\n",
              "        [-2.56741531e-02, -1.49402246e-02,  2.33214833e-02,\n",
              "          1.47563927e-02, -5.89953735e-03,  1.74203031e-02,\n",
              "          2.31558345e-02,  2.75357403e-02,  5.30923530e-03,\n",
              "          3.05856504e-02,  4.10768427e-02, -4.29550521e-02,\n",
              "         -1.84963457e-02, -1.68258771e-02, -3.64640355e-02,\n",
              "          1.77384652e-02],\n",
              "        [ 1.63526200e-02,  9.47099924e-03,  3.39588262e-02,\n",
              "          2.92163827e-02,  2.86422856e-02,  3.78899463e-02,\n",
              "          3.63722183e-02, -9.21551138e-03, -7.35497475e-03,\n",
              "         -4.43332195e-02,  4.92585637e-02, -1.33324973e-02,\n",
              "         -4.37143445e-02,  1.15736946e-02, -3.95272747e-02,\n",
              "          3.49321477e-02],\n",
              "        [-2.09878758e-03, -5.43249771e-03, -3.28605995e-02,\n",
              "         -4.40841913e-03,  1.73928626e-02,  9.19977576e-03,\n",
              "         -2.20934507e-02, -8.33122805e-03,  1.72860883e-02,\n",
              "          3.86811011e-02, -4.37621363e-02, -1.91553123e-02,\n",
              "         -9.16697085e-04, -1.29066817e-02,  2.78218053e-02,\n",
              "         -3.42728645e-02],\n",
              "        [-8.85324553e-03,  2.37769522e-02,  4.83731367e-02,\n",
              "         -1.04464889e-02,  2.86392011e-02, -4.45753708e-02,\n",
              "         -4.22137864e-02, -3.25583816e-02, -1.53252482e-02,\n",
              "         -5.26344776e-03, -2.26006862e-02, -1.53291933e-02,\n",
              "         -2.75295507e-02,  2.27871649e-02,  4.15510796e-02,\n",
              "          3.52303721e-02],\n",
              "        [-2.10707430e-02, -2.29956992e-02,  2.38091685e-02,\n",
              "         -4.63832989e-02, -4.24166694e-02, -4.39635031e-02,\n",
              "         -3.27060372e-02, -9.22756270e-03, -1.80597194e-02,\n",
              "         -8.12790543e-03, -3.33116651e-02,  4.37849276e-02,\n",
              "         -2.87122652e-03,  2.16909163e-02, -1.77807584e-02,\n",
              "         -1.68616176e-02],\n",
              "        [ 1.42425932e-02,  4.40932997e-02, -3.05520371e-03,\n",
              "          2.21633799e-02, -1.94448121e-02, -4.94686142e-02,\n",
              "         -7.41868094e-03, -9.70313698e-03,  1.30837075e-02,\n",
              "         -3.60290185e-02,  4.91077937e-02,  3.08013596e-02,\n",
              "         -9.53118876e-03, -1.62724033e-02, -2.69920826e-02,\n",
              "         -8.98970291e-03],\n",
              "        [-6.01946190e-03,  2.10036747e-02,  1.80919655e-02,\n",
              "          1.77763961e-02, -2.73606908e-02,  4.33437489e-02,\n",
              "         -2.96149384e-02,  2.43267454e-02,  1.52512528e-02,\n",
              "          4.97774743e-02, -2.29999069e-02,  2.56157406e-02,\n",
              "         -1.84502453e-03, -4.48712967e-02, -2.55035050e-02,\n",
              "          2.43267305e-02],\n",
              "        [ 1.10676996e-02, -2.82762889e-02, -3.47631685e-02,\n",
              "          3.07752602e-02, -3.34916115e-02, -3.04891709e-02,\n",
              "         -3.66186276e-02,  1.50977485e-02, -1.30045190e-02,\n",
              "         -2.86408514e-03,  7.28573650e-03, -2.95403004e-02,\n",
              "          9.88300890e-03, -5.91854006e-03, -2.01739799e-02,\n",
              "          4.60788868e-02],\n",
              "        [-2.24040505e-02,  3.52192670e-04,  4.83251847e-02,\n",
              "         -2.40481738e-02, -4.56330180e-02, -3.71617302e-02,\n",
              "          4.50562499e-02,  1.02580674e-02,  1.16337426e-02,\n",
              "          4.40545566e-02, -7.98521191e-03,  3.03262584e-02,\n",
              "         -7.39761442e-03, -4.10532355e-02,  1.10864639e-03,\n",
              "          3.32539119e-02],\n",
              "        [ 1.84208862e-02,  4.30846550e-02,  2.05591433e-02,\n",
              "         -4.14718166e-02,  2.06956752e-02, -2.20575090e-02,\n",
              "          4.80650179e-02,  1.72057264e-02,  2.24200375e-02,\n",
              "         -2.00757142e-02,  4.40195911e-02, -1.15828030e-02,\n",
              "          4.03334238e-02, -2.74635684e-02,  2.80677192e-02,\n",
              "         -3.88440117e-02],\n",
              "        [-4.33663838e-02,  2.97494270e-02,  3.23308446e-02,\n",
              "          1.50817148e-02,  3.05400752e-02, -2.11293101e-02,\n",
              "          4.02062796e-02, -1.05340965e-02, -1.43397599e-04,\n",
              "          2.55982019e-02,  4.79245074e-02, -1.33313164e-02,\n",
              "         -4.76658009e-02,  2.63265036e-02,  2.53945477e-02,\n",
              "          3.99546660e-02],\n",
              "        [-2.41002440e-02, -8.25997442e-03, -4.65115681e-02,\n",
              "          3.07257064e-02,  1.40419640e-02,  3.81151587e-03,\n",
              "         -2.89516207e-02,  2.71257646e-02, -3.26000936e-02,\n",
              "         -5.47729433e-04, -1.84501335e-03, -2.73059253e-02,\n",
              "         -3.01795136e-02, -7.66049698e-03,  4.51110117e-02,\n",
              "          2.50627883e-02],\n",
              "        [ 1.58856399e-02, -2.55652517e-03, -3.21303383e-02,\n",
              "         -4.30661328e-02,  2.19463445e-02,  1.17034428e-02,\n",
              "          7.56683201e-03,  1.12591498e-02,  4.55159061e-02,\n",
              "          3.90288495e-02,  4.39880975e-02, -4.32153232e-02,\n",
              "         -4.12801653e-03,  2.55024917e-02,  1.00478530e-02,\n",
              "          1.46299861e-02],\n",
              "        [ 1.09652281e-02, -1.57476068e-02,  5.01453876e-04,\n",
              "          1.19150877e-02,  3.52969058e-02, -1.02967620e-02,\n",
              "         -2.30931118e-03, -3.01728491e-02,  2.30754875e-02,\n",
              "         -3.41749191e-03,  8.92024115e-03,  8.53861496e-03,\n",
              "          4.33088280e-02,  9.72293317e-04, -1.58842653e-03,\n",
              "          2.41263621e-02],\n",
              "        [ 3.53943743e-02,  5.30546904e-03, -3.66783626e-02,\n",
              "         -2.14957595e-02, -3.70207205e-02, -2.20478065e-02,\n",
              "         -4.46529314e-03,  1.87817253e-02, -2.93290019e-02,\n",
              "          1.75865926e-02,  8.20877403e-03,  9.91144031e-03,\n",
              "         -1.25518329e-02, -4.09890041e-02, -1.55772567e-02,\n",
              "         -1.51118748e-02],\n",
              "        [ 1.17713921e-02, -6.66964799e-04,  1.12964138e-02,\n",
              "         -1.15745179e-02,  4.12516035e-02, -1.29240528e-02,\n",
              "          2.29977071e-04,  4.68624346e-02,  3.97346281e-02,\n",
              "          6.18330389e-03, -8.39098543e-03,  2.72156931e-02,\n",
              "          4.79696728e-02, -2.56248116e-02, -1.99604761e-02,\n",
              "         -1.40210390e-02],\n",
              "        [-4.87588421e-02, -3.02385818e-02, -3.70510928e-02,\n",
              "          3.86484303e-02, -1.95786711e-02, -8.11052322e-03,\n",
              "         -3.76285240e-03, -2.64574420e-02, -8.15361738e-03,\n",
              "         -3.25238593e-02, -4.96943109e-02,  3.16553190e-03,\n",
              "         -9.76129621e-03,  3.80913280e-02,  3.99002172e-02,\n",
              "          1.40929855e-02],\n",
              "        [-9.73821804e-03, -1.82771683e-02, -3.97667773e-02,\n",
              "         -3.10589802e-02, -1.78387761e-02,  4.56898287e-03,\n",
              "         -4.84265685e-02,  2.19968706e-03,  7.18228891e-03,\n",
              "         -4.61898558e-02, -1.18952990e-02,  2.88684107e-02,\n",
              "          1.56372078e-02, -9.32649523e-03,  4.10430916e-02,\n",
              "         -3.67068760e-02],\n",
              "        [-1.82626732e-02,  3.14646028e-02,  3.48735340e-02,\n",
              "          2.99766175e-02,  2.77017020e-02, -1.35107152e-02,\n",
              "          1.09689236e-02, -4.84564193e-02,  4.48250063e-02,\n",
              "         -1.81973353e-02, -4.70874570e-02, -4.30735126e-02,\n",
              "          4.33010347e-02, -7.34217465e-05,  3.66055109e-02,\n",
              "          3.23623084e-02],\n",
              "        [-1.56503431e-02, -1.67927630e-02, -3.47030275e-02,\n",
              "         -3.31514850e-02, -3.85831222e-02, -3.58455293e-02,\n",
              "         -1.37913339e-02,  7.27538019e-03,  1.89796425e-02,\n",
              "         -7.37397745e-03, -7.18270615e-03,  3.26300971e-02,\n",
              "          1.45450570e-02,  3.96744944e-02, -4.93013747e-02,\n",
              "         -4.42283414e-02],\n",
              "        [ 9.28566605e-03,  4.44138311e-02, -3.30781937e-03,\n",
              "          4.48369496e-02,  1.86151154e-02,  4.84949388e-02,\n",
              "         -3.26441601e-03,  4.25900929e-02, -4.46184762e-02,\n",
              "         -1.02510303e-03, -9.44770500e-03, -1.18092299e-02,\n",
              "         -3.61629575e-03,  7.02727586e-04,  2.32199579e-03,\n",
              "         -1.40619166e-02],\n",
              "        [ 4.15964834e-02,  2.49280445e-02, -3.13871987e-02,\n",
              "          1.25865974e-02,  1.98772438e-02, -2.56155729e-02,\n",
              "          2.55988128e-02, -1.75522789e-02,  2.71003135e-02,\n",
              "          1.14586204e-03,  1.29307769e-02, -4.42518592e-02,\n",
              "          4.21356298e-02, -4.36182506e-02,  3.96985151e-02,\n",
              "         -4.65541594e-02],\n",
              "        [-1.46664754e-02,  4.98974808e-02, -3.49237919e-02,\n",
              "         -3.45788486e-02, -3.22773829e-02,  2.33955421e-02,\n",
              "         -1.79603212e-02,  4.93690856e-02,  4.66281287e-02,\n",
              "         -7.10483640e-03, -2.45111939e-02, -3.22934873e-02,\n",
              "          2.50804164e-02, -2.76095271e-02,  1.40489377e-02,\n",
              "         -2.50268932e-02],\n",
              "        [ 1.49764307e-02, -1.40963905e-02, -2.21974608e-02,\n",
              "          3.92702706e-02,  3.87290381e-02,  4.11133803e-02,\n",
              "         -1.23023614e-02, -3.58072892e-02, -1.26762390e-02,\n",
              "          2.02823430e-04, -1.57641061e-02,  4.55770232e-02,\n",
              "          2.30595134e-02,  2.12840475e-02, -2.74720546e-02,\n",
              "          1.69190019e-03],\n",
              "        [-3.50921042e-02,  3.11290883e-02, -7.77880102e-03,\n",
              "          4.44721095e-02, -4.20893654e-02,  3.86141203e-02,\n",
              "          5.17859310e-03,  4.20180894e-02, -4.35779467e-02,\n",
              "          9.09168646e-03, -2.19657421e-02, -4.14970629e-02,\n",
              "         -4.68937159e-02, -4.84866388e-02, -1.70654282e-02,\n",
              "          4.51975204e-02],\n",
              "        [-4.55101617e-02, -2.19556335e-02,  1.19888894e-02,\n",
              "          5.93842193e-03,  2.43462957e-02, -1.19351260e-02,\n",
              "          1.90284289e-02, -4.66668718e-02, -3.26929241e-03,\n",
              "          4.68208641e-03, -1.00234523e-02, -4.29252535e-03,\n",
              "          1.27316974e-02, -4.96952645e-02, -2.37382408e-02,\n",
              "          7.07190111e-03],\n",
              "        [-8.59334320e-03, -1.85001716e-02, -2.14095246e-02,\n",
              "         -8.18954781e-03,  1.04614608e-02,  4.60083224e-02,\n",
              "         -4.20027487e-02, -1.36837848e-02, -8.04395601e-03,\n",
              "         -2.52780207e-02,  3.85099091e-02,  9.65708494e-03,\n",
              "         -3.32772881e-02, -9.42205265e-03,  2.17178799e-02,\n",
              "         -1.37990825e-02],\n",
              "        [ 3.79283540e-02,  2.76262499e-02,  4.48379256e-02,\n",
              "         -1.25990622e-02, -2.86642313e-02, -1.87411085e-02,\n",
              "          1.39716528e-02, -1.76436678e-02, -3.12719122e-02,\n",
              "          4.50558588e-03,  4.82913107e-03, -4.30121534e-02,\n",
              "          3.63855697e-02,  6.31571934e-03, -2.56883986e-02,\n",
              "          1.02244318e-04],\n",
              "        [ 9.31484625e-03, -3.89359482e-02,  1.56846382e-02,\n",
              "          9.85433906e-03, -2.23735925e-02,  4.03030552e-02,\n",
              "         -4.64731716e-02, -2.23091729e-02, -4.66496125e-02,\n",
              "         -3.21469679e-02, -4.34906408e-03,  2.89143808e-02,\n",
              "         -1.24758482e-02, -1.25816837e-02,  2.21988000e-02,\n",
              "         -2.60032173e-02]], dtype=float32)>,\n",
              " <tf.Variable 'dense_1/bias:0' shape=(16,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       dtype=float32)>,\n",
              " <tf.Variable 'dense_2/kernel:0' shape=(16, 16) dtype=float32, numpy=\n",
              " array([[ 0.04775802, -0.01036156, -0.0449517 , -0.02477443, -0.03859916,\n",
              "          0.00439661,  0.04835132,  0.01780519,  0.01648523,  0.01555743,\n",
              "          0.00251657,  0.03800288,  0.04391452, -0.02341455, -0.04489428,\n",
              "          0.01370218],\n",
              "        [ 0.0237042 ,  0.01285079,  0.04879815, -0.0312036 , -0.02777882,\n",
              "          0.04617033,  0.04970068, -0.04811839, -0.01715305, -0.04966554,\n",
              "         -0.00113153,  0.00566176,  0.01188098, -0.03080788,  0.00824678,\n",
              "         -0.03376377],\n",
              "        [-0.01918485,  0.04299   ,  0.01156312,  0.01700941,  0.04584131,\n",
              "          0.03984697,  0.01394099,  0.04655199, -0.00548046,  0.04124236,\n",
              "          0.02913915,  0.01627055,  0.00742415,  0.02127561, -0.02006462,\n",
              "          0.01046588],\n",
              "        [-0.01606315,  0.00541772, -0.01697881, -0.01351812,  0.02071982,\n",
              "         -0.02430201, -0.01839763, -0.00312246, -0.02517308,  0.02777607,\n",
              "         -0.0343708 ,  0.02266128,  0.0148539 , -0.00257831,  0.04496545,\n",
              "         -0.04732108],\n",
              "        [-0.04824093, -0.02628767, -0.02100911,  0.01407239, -0.02124597,\n",
              "          0.03783515, -0.0312325 ,  0.04392381, -0.00992148, -0.03511078,\n",
              "          0.030756  , -0.01947653,  0.0259294 ,  0.0273314 ,  0.04744065,\n",
              "          0.04917021],\n",
              "        [-0.00959679,  0.00399313, -0.02851637,  0.04058013, -0.04424038,\n",
              "         -0.01814969,  0.04072317,  0.04433242, -0.03214319, -0.03967633,\n",
              "         -0.01630181, -0.03286821,  0.04041275,  0.04976442, -0.01137699,\n",
              "          0.03049539],\n",
              "        [ 0.01204833, -0.01614213,  0.01622612, -0.01311529,  0.04636148,\n",
              "          0.01019493, -0.02283794, -0.04985099, -0.00777065,  0.00625052,\n",
              "          0.00793298,  0.02660846,  0.02814417, -0.03720471, -0.00502966,\n",
              "         -0.04915326],\n",
              "        [ 0.02241088,  0.01726918,  0.02651019,  0.03048581,  0.00453955,\n",
              "         -0.04838803, -0.02864581,  0.03691772,  0.01799185, -0.04096822,\n",
              "         -0.00773511,  0.03534192, -0.03452141, -0.02919675, -0.01711857,\n",
              "          0.03827157],\n",
              "        [-0.00505558, -0.04863843, -0.03159948,  0.0381918 ,  0.03746987,\n",
              "          0.03179088,  0.03859406, -0.01950139,  0.00115933,  0.03962764,\n",
              "          0.02713105,  0.04130055,  0.04498145,  0.02177497,  0.02784655,\n",
              "         -0.03035712],\n",
              "        [ 0.04954434,  0.04106269,  0.02302948,  0.0279168 ,  0.00724173,\n",
              "         -0.0074478 ,  0.04024084, -0.01665366, -0.01045825, -0.01368558,\n",
              "         -0.0416463 ,  0.04229862,  0.03670489,  0.00995921,  0.00873436,\n",
              "         -0.00783474],\n",
              "        [ 0.03622607,  0.02842723, -0.01061667, -0.01781498, -0.01141448,\n",
              "          0.04372264,  0.04001142, -0.00979862,  0.02046076, -0.04419238,\n",
              "          0.03056775, -0.00425224,  0.04669834,  0.03929429, -0.00259935,\n",
              "         -0.0339477 ],\n",
              "        [ 0.01723024,  0.04037143,  0.04273823,  0.01575181,  0.03930649,\n",
              "         -0.02503272, -0.01936002,  0.04257968,  0.03060391,  0.04143855,\n",
              "         -0.03182529, -0.01976313,  0.04387218, -0.00337033, -0.03681169,\n",
              "         -0.02276596],\n",
              "        [-0.0298566 , -0.04197897,  0.04849012, -0.02593594,  0.00514711,\n",
              "         -0.03527523,  0.00712538, -0.03335658, -0.04980818,  0.02438435,\n",
              "          0.01038021,  0.01895846,  0.04961345,  0.03809906, -0.00187476,\n",
              "          0.03772912],\n",
              "        [-0.04614392, -0.00311428,  0.04840967,  0.02036696, -0.04812263,\n",
              "          0.03852409, -0.00569602, -0.03006518,  0.02678635, -0.01184653,\n",
              "          0.02158853, -0.01320397, -0.04336245,  0.02366101,  0.01288516,\n",
              "         -0.00858463],\n",
              "        [ 0.03016403,  0.03402109, -0.0429594 ,  0.01160336, -0.00592338,\n",
              "          0.01532158,  0.00935994, -0.03376176, -0.02067286, -0.01587235,\n",
              "          0.0204559 , -0.01643754, -0.03823106, -0.04179012, -0.03797495,\n",
              "         -0.02625991],\n",
              "        [-0.00875927, -0.03096306, -0.01201118, -0.0032308 ,  0.00605261,\n",
              "         -0.03415293, -0.01955097, -0.02038292, -0.00208061,  0.0099049 ,\n",
              "          0.01824001,  0.03665544, -0.02770763,  0.04115743,  0.02469201,\n",
              "         -0.01541052]], dtype=float32)>,\n",
              " <tf.Variable 'dense_2/bias:0' shape=(16,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       dtype=float32)>,\n",
              " <tf.Variable 'dense_3/kernel:0' shape=(16, 1) dtype=float32, numpy=\n",
              " array([[-0.0132513 ],\n",
              "        [-0.0252443 ],\n",
              "        [-0.01716807],\n",
              "        [ 0.01782623],\n",
              "        [-0.04038595],\n",
              "        [-0.01631244],\n",
              "        [ 0.00447171],\n",
              "        [-0.03962324],\n",
              "        [ 0.00341314],\n",
              "        [-0.04871621],\n",
              "        [ 0.02474498],\n",
              "        [-0.00339692],\n",
              "        [-0.04762357],\n",
              "        [-0.01059543],\n",
              "        [ 0.01924895],\n",
              "        [ 0.01293265]], dtype=float32)>,\n",
              " <tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YVYzb8syqB7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fcade5f7-d9ff-4503-fe8e-4c8ac55e6071"
      },
      "source": [
        "classifier.fit(X_train,Y_train,batch_size=90,nb_epoch=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "426/426 [==============================] - 0s 708us/step - loss: 0.6924 - accuracy: 0.6174\n",
            "Epoch 2/200\n",
            "426/426 [==============================] - 0s 25us/step - loss: 0.6902 - accuracy: 0.6268\n",
            "Epoch 3/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.6869 - accuracy: 0.6268\n",
            "Epoch 4/200\n",
            "426/426 [==============================] - 0s 25us/step - loss: 0.6817 - accuracy: 0.6268\n",
            "Epoch 5/200\n",
            "426/426 [==============================] - 0s 28us/step - loss: 0.6741 - accuracy: 0.6291\n",
            "Epoch 6/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.6626 - accuracy: 0.6408\n",
            "Epoch 7/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.6476 - accuracy: 0.7019\n",
            "Epoch 8/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.6270 - accuracy: 0.7958\n",
            "Epoch 9/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.6007 - accuracy: 0.8850\n",
            "Epoch 10/200\n",
            "426/426 [==============================] - 0s 30us/step - loss: 0.5686 - accuracy: 0.9202\n",
            "Epoch 11/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.5313 - accuracy: 0.9390\n",
            "Epoch 12/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.4901 - accuracy: 0.9460\n",
            "Epoch 13/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.4463 - accuracy: 0.9507\n",
            "Epoch 14/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.4033 - accuracy: 0.9507\n",
            "Epoch 15/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.3620 - accuracy: 0.9531\n",
            "Epoch 16/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.3233 - accuracy: 0.9531\n",
            "Epoch 17/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.2890 - accuracy: 0.9554\n",
            "Epoch 18/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.2580 - accuracy: 0.9601\n",
            "Epoch 19/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.2303 - accuracy: 0.9624\n",
            "Epoch 20/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.2068 - accuracy: 0.9624\n",
            "Epoch 21/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.1877 - accuracy: 0.9695\n",
            "Epoch 22/200\n",
            "426/426 [==============================] - 0s 25us/step - loss: 0.1703 - accuracy: 0.9718\n",
            "Epoch 23/200\n",
            "426/426 [==============================] - 0s 33us/step - loss: 0.1566 - accuracy: 0.9718\n",
            "Epoch 24/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.1444 - accuracy: 0.9765\n",
            "Epoch 25/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.1344 - accuracy: 0.9765\n",
            "Epoch 26/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.1255 - accuracy: 0.9765\n",
            "Epoch 27/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.1183 - accuracy: 0.9765\n",
            "Epoch 28/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.1124 - accuracy: 0.9765\n",
            "Epoch 29/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.1066 - accuracy: 0.9765\n",
            "Epoch 30/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.1021 - accuracy: 0.9765\n",
            "Epoch 31/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0978 - accuracy: 0.9765\n",
            "Epoch 32/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0942 - accuracy: 0.9789\n",
            "Epoch 33/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0910 - accuracy: 0.9789\n",
            "Epoch 34/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0881 - accuracy: 0.9812\n",
            "Epoch 35/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0855 - accuracy: 0.9812\n",
            "Epoch 36/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0833 - accuracy: 0.9812\n",
            "Epoch 37/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0811 - accuracy: 0.9836\n",
            "Epoch 38/200\n",
            "426/426 [==============================] - 0s 25us/step - loss: 0.0792 - accuracy: 0.9859\n",
            "Epoch 39/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0776 - accuracy: 0.9859\n",
            "Epoch 40/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0760 - accuracy: 0.9859\n",
            "Epoch 41/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0746 - accuracy: 0.9859\n",
            "Epoch 42/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0733 - accuracy: 0.9859\n",
            "Epoch 43/200\n",
            "426/426 [==============================] - 0s 20us/step - loss: 0.0721 - accuracy: 0.9859\n",
            "Epoch 44/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0710 - accuracy: 0.9859\n",
            "Epoch 45/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0701 - accuracy: 0.9859\n",
            "Epoch 46/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0690 - accuracy: 0.9859\n",
            "Epoch 47/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0681 - accuracy: 0.9859\n",
            "Epoch 48/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0672 - accuracy: 0.9859\n",
            "Epoch 49/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0664 - accuracy: 0.9859\n",
            "Epoch 50/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0656 - accuracy: 0.9859\n",
            "Epoch 51/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0648 - accuracy: 0.9859\n",
            "Epoch 52/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0641 - accuracy: 0.9859\n",
            "Epoch 53/200\n",
            "426/426 [==============================] - 0s 46us/step - loss: 0.0633 - accuracy: 0.9859\n",
            "Epoch 54/200\n",
            "426/426 [==============================] - 0s 34us/step - loss: 0.0626 - accuracy: 0.9859\n",
            "Epoch 55/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0621 - accuracy: 0.9859\n",
            "Epoch 56/200\n",
            "426/426 [==============================] - 0s 26us/step - loss: 0.0613 - accuracy: 0.9859\n",
            "Epoch 57/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0606 - accuracy: 0.9859\n",
            "Epoch 58/200\n",
            "426/426 [==============================] - 0s 28us/step - loss: 0.0601 - accuracy: 0.9859\n",
            "Epoch 59/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0595 - accuracy: 0.9859\n",
            "Epoch 60/200\n",
            "426/426 [==============================] - 0s 19us/step - loss: 0.0589 - accuracy: 0.9859\n",
            "Epoch 61/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0583 - accuracy: 0.9859\n",
            "Epoch 62/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0578 - accuracy: 0.9859\n",
            "Epoch 63/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0573 - accuracy: 0.9859\n",
            "Epoch 64/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0568 - accuracy: 0.9883\n",
            "Epoch 65/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.0563 - accuracy: 0.9883\n",
            "Epoch 66/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0558 - accuracy: 0.9883\n",
            "Epoch 67/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0554 - accuracy: 0.9883\n",
            "Epoch 68/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0550 - accuracy: 0.9883\n",
            "Epoch 69/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0545 - accuracy: 0.9883\n",
            "Epoch 70/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0541 - accuracy: 0.9883\n",
            "Epoch 71/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0537 - accuracy: 0.9883\n",
            "Epoch 72/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0533 - accuracy: 0.9883\n",
            "Epoch 73/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0529 - accuracy: 0.9883\n",
            "Epoch 74/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0525 - accuracy: 0.9883\n",
            "Epoch 75/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0521 - accuracy: 0.9883\n",
            "Epoch 76/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0517 - accuracy: 0.9883\n",
            "Epoch 77/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.0514 - accuracy: 0.9883\n",
            "Epoch 78/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.0510 - accuracy: 0.9883\n",
            "Epoch 79/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0507 - accuracy: 0.9883\n",
            "Epoch 80/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0503 - accuracy: 0.9883\n",
            "Epoch 81/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0500 - accuracy: 0.9883\n",
            "Epoch 82/200\n",
            "426/426 [==============================] - 0s 18us/step - loss: 0.0496 - accuracy: 0.9883\n",
            "Epoch 83/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0492 - accuracy: 0.9883\n",
            "Epoch 84/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0489 - accuracy: 0.9883\n",
            "Epoch 85/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0485 - accuracy: 0.9883\n",
            "Epoch 86/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0482 - accuracy: 0.9883\n",
            "Epoch 87/200\n",
            "426/426 [==============================] - 0s 26us/step - loss: 0.0480 - accuracy: 0.9883\n",
            "Epoch 88/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0475 - accuracy: 0.9883\n",
            "Epoch 89/200\n",
            "426/426 [==============================] - 0s 20us/step - loss: 0.0472 - accuracy: 0.9883\n",
            "Epoch 90/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0469 - accuracy: 0.9883\n",
            "Epoch 91/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0467 - accuracy: 0.9883\n",
            "Epoch 92/200\n",
            "426/426 [==============================] - 0s 25us/step - loss: 0.0463 - accuracy: 0.9883\n",
            "Epoch 93/200\n",
            "426/426 [==============================] - 0s 19us/step - loss: 0.0460 - accuracy: 0.9883\n",
            "Epoch 94/200\n",
            "426/426 [==============================] - 0s 20us/step - loss: 0.0457 - accuracy: 0.9883\n",
            "Epoch 95/200\n",
            "426/426 [==============================] - 0s 19us/step - loss: 0.0454 - accuracy: 0.9883\n",
            "Epoch 96/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0451 - accuracy: 0.9883\n",
            "Epoch 97/200\n",
            "426/426 [==============================] - 0s 37us/step - loss: 0.0448 - accuracy: 0.9906\n",
            "Epoch 98/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0445 - accuracy: 0.9906\n",
            "Epoch 99/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0442 - accuracy: 0.9906\n",
            "Epoch 100/200\n",
            "426/426 [==============================] - 0s 26us/step - loss: 0.0440 - accuracy: 0.9906\n",
            "Epoch 101/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.0436 - accuracy: 0.9906\n",
            "Epoch 102/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0434 - accuracy: 0.9906\n",
            "Epoch 103/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0431 - accuracy: 0.9906\n",
            "Epoch 104/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0429 - accuracy: 0.9906\n",
            "Epoch 105/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0425 - accuracy: 0.9906\n",
            "Epoch 106/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0422 - accuracy: 0.9906\n",
            "Epoch 107/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0420 - accuracy: 0.9906\n",
            "Epoch 108/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0416 - accuracy: 0.9906\n",
            "Epoch 109/200\n",
            "426/426 [==============================] - 0s 20us/step - loss: 0.0413 - accuracy: 0.9906\n",
            "Epoch 110/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0411 - accuracy: 0.9906\n",
            "Epoch 111/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0408 - accuracy: 0.9906\n",
            "Epoch 112/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0405 - accuracy: 0.9906\n",
            "Epoch 113/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0402 - accuracy: 0.9906\n",
            "Epoch 114/200\n",
            "426/426 [==============================] - 0s 25us/step - loss: 0.0399 - accuracy: 0.9906\n",
            "Epoch 115/200\n",
            "426/426 [==============================] - 0s 20us/step - loss: 0.0396 - accuracy: 0.9906\n",
            "Epoch 116/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.0394 - accuracy: 0.9906\n",
            "Epoch 117/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0391 - accuracy: 0.9906\n",
            "Epoch 118/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0388 - accuracy: 0.9906\n",
            "Epoch 119/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0386 - accuracy: 0.9906\n",
            "Epoch 120/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.0383 - accuracy: 0.9906\n",
            "Epoch 121/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0380 - accuracy: 0.9906\n",
            "Epoch 122/200\n",
            "426/426 [==============================] - 0s 29us/step - loss: 0.0377 - accuracy: 0.9906\n",
            "Epoch 123/200\n",
            "426/426 [==============================] - 0s 25us/step - loss: 0.0375 - accuracy: 0.9906\n",
            "Epoch 124/200\n",
            "426/426 [==============================] - 0s 25us/step - loss: 0.0372 - accuracy: 0.9906\n",
            "Epoch 125/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0369 - accuracy: 0.9906\n",
            "Epoch 126/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0368 - accuracy: 0.9906\n",
            "Epoch 127/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0365 - accuracy: 0.9906\n",
            "Epoch 128/200\n",
            "426/426 [==============================] - 0s 20us/step - loss: 0.0362 - accuracy: 0.9906\n",
            "Epoch 129/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0360 - accuracy: 0.9906\n",
            "Epoch 130/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0357 - accuracy: 0.9906\n",
            "Epoch 131/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0355 - accuracy: 0.9906\n",
            "Epoch 132/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0353 - accuracy: 0.9906\n",
            "Epoch 133/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0350 - accuracy: 0.9906\n",
            "Epoch 134/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0347 - accuracy: 0.9906\n",
            "Epoch 135/200\n",
            "426/426 [==============================] - 0s 26us/step - loss: 0.0345 - accuracy: 0.9906\n",
            "Epoch 136/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.0344 - accuracy: 0.9906\n",
            "Epoch 137/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0340 - accuracy: 0.9906\n",
            "Epoch 138/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0337 - accuracy: 0.9906\n",
            "Epoch 139/200\n",
            "426/426 [==============================] - 0s 25us/step - loss: 0.0336 - accuracy: 0.9906\n",
            "Epoch 140/200\n",
            "426/426 [==============================] - 0s 42us/step - loss: 0.0333 - accuracy: 0.9906\n",
            "Epoch 141/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0331 - accuracy: 0.9906\n",
            "Epoch 142/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0329 - accuracy: 0.9906\n",
            "Epoch 143/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.0326 - accuracy: 0.9906\n",
            "Epoch 144/200\n",
            "426/426 [==============================] - 0s 26us/step - loss: 0.0325 - accuracy: 0.9906\n",
            "Epoch 145/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0323 - accuracy: 0.9906\n",
            "Epoch 146/200\n",
            "426/426 [==============================] - 0s 19us/step - loss: 0.0321 - accuracy: 0.9906\n",
            "Epoch 147/200\n",
            "426/426 [==============================] - 0s 35us/step - loss: 0.0319 - accuracy: 0.9906\n",
            "Epoch 148/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0316 - accuracy: 0.9906\n",
            "Epoch 149/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0315 - accuracy: 0.9906\n",
            "Epoch 150/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.0313 - accuracy: 0.9906\n",
            "Epoch 151/200\n",
            "426/426 [==============================] - 0s 28us/step - loss: 0.0311 - accuracy: 0.9906\n",
            "Epoch 152/200\n",
            "426/426 [==============================] - 0s 30us/step - loss: 0.0308 - accuracy: 0.9906\n",
            "Epoch 153/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0306 - accuracy: 0.9906\n",
            "Epoch 154/200\n",
            "426/426 [==============================] - 0s 30us/step - loss: 0.0305 - accuracy: 0.9906\n",
            "Epoch 155/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0303 - accuracy: 0.9906\n",
            "Epoch 156/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0301 - accuracy: 0.9906\n",
            "Epoch 157/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0299 - accuracy: 0.9906\n",
            "Epoch 158/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0297 - accuracy: 0.9906\n",
            "Epoch 159/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0296 - accuracy: 0.9906\n",
            "Epoch 160/200\n",
            "426/426 [==============================] - 0s 26us/step - loss: 0.0293 - accuracy: 0.9906\n",
            "Epoch 161/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0292 - accuracy: 0.9906\n",
            "Epoch 162/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0290 - accuracy: 0.9906\n",
            "Epoch 163/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0288 - accuracy: 0.9906\n",
            "Epoch 164/200\n",
            "426/426 [==============================] - 0s 28us/step - loss: 0.0287 - accuracy: 0.9906\n",
            "Epoch 165/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0285 - accuracy: 0.9906\n",
            "Epoch 166/200\n",
            "426/426 [==============================] - 0s 25us/step - loss: 0.0283 - accuracy: 0.9906\n",
            "Epoch 167/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0282 - accuracy: 0.9906\n",
            "Epoch 168/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0280 - accuracy: 0.9906\n",
            "Epoch 169/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0277 - accuracy: 0.9906\n",
            "Epoch 170/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0275 - accuracy: 0.9906\n",
            "Epoch 171/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0274 - accuracy: 0.9906\n",
            "Epoch 172/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0272 - accuracy: 0.9906\n",
            "Epoch 173/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.0270 - accuracy: 0.9906\n",
            "Epoch 174/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0268 - accuracy: 0.9906\n",
            "Epoch 175/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0267 - accuracy: 0.9906\n",
            "Epoch 176/200\n",
            "426/426 [==============================] - 0s 20us/step - loss: 0.0265 - accuracy: 0.9906\n",
            "Epoch 177/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0263 - accuracy: 0.9906\n",
            "Epoch 178/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0262 - accuracy: 0.9906\n",
            "Epoch 179/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0260 - accuracy: 0.9906\n",
            "Epoch 180/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0258 - accuracy: 0.9906\n",
            "Epoch 181/200\n",
            "426/426 [==============================] - 0s 25us/step - loss: 0.0256 - accuracy: 0.9906\n",
            "Epoch 182/200\n",
            "426/426 [==============================] - 0s 25us/step - loss: 0.0255 - accuracy: 0.9906\n",
            "Epoch 183/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0254 - accuracy: 0.9906\n",
            "Epoch 184/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0252 - accuracy: 0.9906\n",
            "Epoch 185/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0250 - accuracy: 0.9906\n",
            "Epoch 186/200\n",
            "426/426 [==============================] - 0s 26us/step - loss: 0.0248 - accuracy: 0.9906\n",
            "Epoch 187/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0246 - accuracy: 0.9906\n",
            "Epoch 188/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0244 - accuracy: 0.9906\n",
            "Epoch 189/200\n",
            "426/426 [==============================] - 0s 20us/step - loss: 0.0243 - accuracy: 0.9906\n",
            "Epoch 190/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0241 - accuracy: 0.9906\n",
            "Epoch 191/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0240 - accuracy: 0.9906\n",
            "Epoch 192/200\n",
            "426/426 [==============================] - 0s 24us/step - loss: 0.0238 - accuracy: 0.9906\n",
            "Epoch 193/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0237 - accuracy: 0.9906\n",
            "Epoch 194/200\n",
            "426/426 [==============================] - 0s 23us/step - loss: 0.0235 - accuracy: 0.9906\n",
            "Epoch 195/200\n",
            "426/426 [==============================] - 0s 21us/step - loss: 0.0233 - accuracy: 0.9906\n",
            "Epoch 196/200\n",
            "426/426 [==============================] - 0s 20us/step - loss: 0.0232 - accuracy: 0.9906\n",
            "Epoch 197/200\n",
            "426/426 [==============================] - 0s 27us/step - loss: 0.0230 - accuracy: 0.9906\n",
            "Epoch 198/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0229 - accuracy: 0.9906\n",
            "Epoch 199/200\n",
            "426/426 [==============================] - 0s 33us/step - loss: 0.0227 - accuracy: 0.9906\n",
            "Epoch 200/200\n",
            "426/426 [==============================] - 0s 22us/step - loss: 0.0225 - accuracy: 0.9906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fbce86c98d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avXnIPVKzJo2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "628e4024-7ff8-40f6-e526-665c71bd3ec5"
      },
      "source": [
        "Y_pred = classifier.predict(X_test)\n",
        "Y_pred  = (Y_pred > 0.5)\n",
        "Y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgoBFBhk04-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Y_test,Y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ofAhbya1GZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "db789225-4f05-4b8a-ef20-f591b96b48be"
      },
      "source": [
        "sns.heatmap(cm,annot=True)\n",
        "plt.savefig('test_data.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPhiou1FJWHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# REMOVE CHANGEING OF ACCURACY AT RUN TIME MODEL\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import sklearn\n",
        "from sklearn.model_selection import cross_val_score  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwMp6CjdKADk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def built_classifier():\n",
        "  classifier = Sequential()\n",
        "  #First Layer (Input + Hidden Layer)\n",
        "  classifier.add(Dense(output_dim = 16,init='uniform',activation='relu',input_dim=30 ))\n",
        "  #Second Layer (Hidden Layer)\n",
        "  classifier.add(Dense(output_dim = 16,init='uniform',activation='relu'))\n",
        "  #Third Layer (Output Layer)\n",
        "  classifier.add(Dense(output_dim = 1,init='uniform',activation=\"sigmoid\" ))\n",
        "  classifier.compile(optimizer=\"Adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
        "  return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtc9dOT8KqRQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8b2fedd4-24b4-45cd-e0e8-1af90770dda0"
      },
      "source": [
        "model = KerasClassifier(build_fn=built_classifier,batch_size=100,epochs=200)\n",
        "accuracies = cross_val_score(estimator=model,X=X_train,y = Y_train,cv=10,n_jobs=-1)\n",
        "accuracies\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.        , 1.        , 0.97674417, 1.        , 0.90697676,\n",
              "       0.95348835, 1.        , 1.        , 0.97619045, 1.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyvrEoPoLsyP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40b942bd-595f-4a65-b17a-f74c3ab4984d"
      },
      "source": [
        "accuracies.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9813399732112884"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HXM0tLgL0Ow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf69dad4-754b-4257-9110-889f54488a43"
      },
      "source": [
        "accuracies.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02905584116076868"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCOTAmEzMC79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}